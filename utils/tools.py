import os
import json
import evaluate
import re
from string2string.similarity import BARTScore
from huggingface_hub import ModelCard, ModelCardData, metadata_update
from blanc import BlancHelp

def get_id_and_version_and_prev_results(evaluation_results_filepath, args):
    """
    Get the model ID, version counter, and previous results.

    Args:
        evaluation_results_filepath (str): The filepath to the evaluation results file.
        args (object): The arguments object containing various parameters.

    Returns:
        tuple: A tuple containing the model ID, version counter, and previous results.
    """

    if os.path.isfile(evaluation_results_filepath):
        with open(evaluation_results_filepath, 'r') as f:
            previous_results = json.load(f)
    else:
        previous_results = []

    version_counter = 1
        
    if args.no_extraction:
        model_id = f"{args.abstractive_model}_no_extraction_V{version_counter}"

        while any(entry["Model_ID"] == model_id for entry in previous_results):
            version_counter += 1
            model_id = f"{args.abstractive_model}_no_extraction_V{version_counter}"
        
        if args.testing_only:
            version_counter -= 1
            model_id = f"{args.abstractive_model}_no_extraction_V{version_counter}"

        return model_id, version_counter, previous_results
    
    model_id = f"{args.extractive_model}_{args.abstractive_model}_{args.mode}"
    if args.mode == "Fixed" or args.mode == "Hybrid":
        model_id += f"_ratio_{args.compression_ratio}"

    model_id += f"_V{version_counter}"

    while any(entry["Model_ID"] == model_id for entry in previous_results):
        version_counter += 1
        model_id = f"{args.extractive_model}_{args.abstractive_model}_{args.mode}"
        if args.mode == "Fixed" or args.mode == "Hybrid":
            model_id += f"_ratio_{args.compression_ratio}"
        model_id += f"_V{version_counter}"

    if args.testing_only:
        version_counter -= 1
        model_id = f"{args.extractive_model}_{args.abstractive_model}_{args.mode}"
        if args.mode == "Fixed" or args.mode == "Hybrid":
            model_id += f"_ratio_{args.compression_ratio}"
        model_id += f"_V{version_counter}"

    return model_id, version_counter, previous_results


def calculate_hybrid_final_step_ratio(intermediate_summary, abstractive_model_token_length, extractive_tokenizer):
    """
    Calculates the final ratio between the token length of the abstractive model and the intermediate summary.
    BEWARE WHEN THE RATIO IS LARGER THAN 1, THE INTERMEDIATE SUMMARY IS LARGER THAN THE ABSTRACTIVE MODEL TOKEN LENGTH.

    Parameters:
    intermediate_summary (str): The intermediate summary generated by the abstractive model.
    abstractive_model_token_length (int): The token length of the abstractive model.
    extractive_tokenizer (object): The tokenizer used for the extractive model.

    Returns:
    float: The final ratio between the token length of the abstractive model and the intermediate summary.
    """

    token_length = extractive_tokenizer(intermediate_summary, return_tensors='pt')['input_ids'].shape[1]
    final_ratio = (abstractive_model_token_length / token_length)

    return final_ratio


def remove_unused_columns(dataset):
    """
    Removes unused columns from the dataset.

    Args:
        dataset (Dataset): The dataset to remove unused columns from.

    Returns:
        Dataset: The dataset with unused columns removed.
    """
    columns_to_keep = ["input_ids", "attention_mask", "labels"]
    all_datasets = ["train", "validation", "test"]
    for dataset_name in all_datasets:
        all_columns = dataset[dataset_name].column_names
        columns_to_remove = [col for col in all_columns if col not in columns_to_keep]
        dataset[dataset_name] = dataset[dataset_name].remove_columns(columns_to_remove)
    return dataset


def calculate_rouge_score(predictions, references):
    """
    Calculates the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scores for a given set of predictions and references.

    Args:
        predictions (list): A list of predicted texts.
        references (list): A list of reference texts.

    Returns:
        dict: A dictionary containing the ROUGE scores for rouge1, rouge2, and rougeL.

    """
    print("Calculating ROUGE scores...")
    rouge_evaluation_metric = evaluate.load('rouge')
    rouge_scores = rouge_evaluation_metric.compute(predictions=predictions, references=references, rouge_types=["rouge1", "rouge2", "rougeL"])

    return rouge_scores


def calculate_bert_score(predictions, references, batch_size=2):
    """
    Calculates the BERT score for a given set of predictions and references.

    Args:
        predictions (list): A list of predicted sentences.
        references (list): A list of reference sentences.

    Returns:
        float: The final BERT score.

    """
    bert_score_evaluation_metric = evaluate.load('bertscore')
    bert_scores = bert_score_evaluation_metric.compute(references=references, predictions=predictions, model_type="allenai/longformer-base-4096", batch_size=batch_size)
    
    final_bert_score = sum(bert_scores['f1']) / len(bert_scores['f1'])

    del bert_score_evaluation_metric

    return final_bert_score


def calculate_bart_score(predictions, references, batch_size=2):
    """
    Calculate the F1 score using the precision and recall scores.

    Args:
        predictions (list): A list of predicted sentences.
        references (list): A list of reference sentences.

    Returns:
        float: The F1 score.
    """
    # Beware, BARTScore is memory intensive and it can't handle texts longer than 1024 tokens.
    bart_score_evaluation_metric = BARTScore(model_name_or_path='facebook/bart-large-cnn', device='cuda')
    bart_score_precision = bart_score_evaluation_metric.compute(source_sentences=references, target_sentences=predictions, batch_size=batch_size)
    bart_score_recall = bart_score_evaluation_metric.compute(source_sentences=predictions, target_sentences=references, batch_size=batch_size)

    precision_scores = bart_score_precision['score']
    recall_scores = bart_score_recall['score']

    f1_scores = []
    for precision, recall in zip(precision_scores, recall_scores):
        f1 = 2 * ((precision * recall) / (precision + recall))
        f1_scores.append(f1)

    f1_score = sum(f1_scores) / len(f1_scores)
    
    del bart_score_evaluation_metric
    
    return f1_score


def calculate_blanc_score(predictions, references, batch_size=2):
    """
    Calculates the BLANC score for a given set of predictions and references.

    Args:
        predictions (list): A list of predicted summaries.
        references (list): A list of reference summaries.

    Returns:
        float: The calculated BLANC score.

    """
    blanc_help = BlancHelp(device='cuda', inference_batch_size=batch_size)
    blanc_scores = blanc_help.eval_pairs(docs=references, summaries=predictions)

    blanc_score = sum(blanc_scores) / len(blanc_scores)

    del blanc_help

    return blanc_score



def read_created_summaries(model_id):
    """
    Read the created summaries from a text file.

    Args:
        model_id (str): The ID of the model.

    Returns:
        list: A list of created summaries.

    """
    # Load the text file content
    file_path = f'results/text_outputs/{model_id}_predictions.txt'
    with open(file_path, 'r', encoding='utf-8') as file:
        text_content = file.read()

    # Regular expression to find all summaries
    pattern = r'Summary \d+:\s(.*?)(?=Summary \d+:|\Z)'

    # Find all matches
    summaries = re.findall(pattern, text_content, re.DOTALL)
    print("Found all summaries")
    pred_str = [summary.strip() for summary in summaries]

    return pred_str
def create_model_card(results):
    """
    Creates a model card based on the provided results. If a template is not found, a default model card is generated, so it is recommended to have a template in the 'docs' directory.

    Args:
        results (dict): A dictionary containing the model evaluation results.

    Returns:
        ModelCard: The generated model card.

    """
    file_path = os.path.join('docs', 'card_template.md')
    content = ''
    if os.path.exists(file_path):
        with open('docs/card_template.md', 'r') as f:
            content += f.read()
        print(results)
        placeholders = {
            'PLACEHOLDER_MODEL_ID': results['Model_ID'],
            'PLACEHOLDER_BASE_MODEL': results['Abstractive_model'],
            'PLACEHOLDER_EXTRACTIVE_MODEL': results['Extractive_model'],
            'PLACEHOLDER_RATIO_MODE': results['Ratio_mode'],
            'PLACEHOLDER_ROUGE1': str(results['Evaluation_metrics']['ROUGE-1']),
            'PLACEHOLDER_ROUGE2': str(results['Evaluation_metrics']['ROUGE-2']),
            'PLACEHOLDER_ROUGEL': str(results['Evaluation_metrics']['ROUGE-L']),
            'PLACEHOLDER_BERTSCORE': str(results['Evaluation_metrics']['BERTScore']),
            'PLACEHOLDER_BARTSCORE': str(results['Evaluation_metrics']['BARTScore']),
            'PLACEHOLDER_BLANC': str(results['Evaluation_metrics']['BLANC'])
        }
        for placeholder, value in placeholders.items():
            content = content.replace(placeholder, value)
        
        card = ModelCard(content)

    else:
        # Placeholder model card data
        card_data = ModelCardData(language='en')
        card = ModelCard.from_template(card_data)

    return card

